{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture Implementation (PyTorch)"
      ],
      "metadata": {
        "id": "hG3xPgfbt_gP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports essential libraries for mathematical operations, type hints, and PyTorch modules to build and manage neural networks."
      ],
      "metadata": {
        "id": "CPBsrVJEk2ol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X820vIltzgy"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaled Dot-Product Attention\n"
      ],
      "metadata": {
        "id": "Qys7ONkazWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Scaled Dot-Product Attention class, which calculates attention scores between queries (Q) and keys (K), applies a mask if provided, normalizes with softmax, and uses the scores to weight the values (V) to produce the final context and attention outputs."
      ],
      "metadata": {
        "id": "m3MyWBQdk8kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"Compute scaled dot-product attention.\n",
        "\n",
        "\n",
        "    Inputs:\n",
        "        Q: (..., seq_len_q, d_k)\n",
        "        K: (..., seq_len_k, d_k)\n",
        "        V: (..., seq_len_v, d_v) where seq_len_k == seq_len_v\n",
        "        mask: (..., seq_len_q, seq_len_k) with 0 for allowed positions and 1 (or -inf) for masked.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        context: (..., seq_len_q, d_v)\n",
        "        attn: (..., seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
        "\n",
        "\n",
        "    def forward(self, Q, K, V, mask: Optional[torch.Tensor] = None):\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k) # (..., seq_q, seq_k)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask should be broadcastable to scores' shape. We use -1e9 for numerical stability.\n",
        "            scores = scores.masked_fill(mask == 0, float('-1e9'))\n",
        "\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        if self.dropout is not None:\n",
        "            attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn"
      ],
      "metadata": {
        "id": "bHZih-S3uFhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention\n"
      ],
      "metadata": {
        "id": "_JC1uQw1zP7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Multi-Head Attention class, which splits queries, keys, and values into multiple heads, applies scaled dot-product attention to each head, then concatenates and linearly transforms the results to capture information from different representation subspaces."
      ],
      "metadata": {
        "id": "ZtbYR574lCWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Attention as in the paper.\n",
        "\n",
        "\n",
        "    Splits Q,K,V into h heads, applies scaled dot-product attention, concatenates heads.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, d_model=512, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.h = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_v = self.d_k\n",
        "\n",
        "\n",
        "        # linear projections\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask: Optional[torch.Tensor] = None):\n",
        "        # query/key/value: (batch, seq_len, d_model)\n",
        "        B = query.size(0)\n",
        "\n",
        "\n",
        "        # linear projections\n",
        "        Q = self.w_q(query) # (B, seq_q, d_model)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "\n",
        "\n",
        "        # split into heads\n",
        "        Q = Q.view(B, -1, self.h, self.d_k).transpose(1, 2) # (B, h, seq_q, d_k)\n",
        "        K = K.view(B, -1, self.h, self.d_k).transpose(1, 2) # (B, h, seq_k, d_k)\n",
        "        V = V.view(B, -1, self.h, self.d_k).transpose(1, 2) # (B, h, seq_v, d_v)\n",
        "\n",
        "\n",
        "        # adjust mask for heads if provided: expected mask shape -> (B, 1, seq_q, seq_k) or broadcastable\n",
        "        if mask is not None:\n",
        "            # ensure mask shape is (B, 1, seq_q, seq_k)\n",
        "            mask = mask.unsqueeze(1) if mask.dim() == 3 else mask\n",
        "\n",
        "\n",
        "        # apply attention on all the projected vectors in batch\n",
        "        context, attn = self.attention(Q, K, V, mask=mask) # context: (B, h, seq_q, d_v)\n",
        "\n",
        "\n",
        "        # concat heads\n",
        "        context = context.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k) # (B, seq_q, d_model)\n",
        "\n",
        "\n",
        "        output = self.w_o(context)\n",
        "        output = self.dropout(output)\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "wcPKfOvpwNnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Position-wise Feed-Forward Network\n"
      ],
      "metadata": {
        "id": "beKi2LpozKYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Position-wise Feed-Forward Network, which applies two linear layers with a ReLU activation and dropout in between to process each position independently in the sequence."
      ],
      "metadata": {
        "id": "nR5FqTKhlH1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPositionwiseFFN(nn.Module):\n",
        "    \"\"\"Two-layer feed-forward network with ReLU activation in between.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "D-9DfP4swbBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "fN73e8GA2ADz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Positional Encoding class, which adds fixed sinusoidal position information to input embeddings so the model can understand the order of tokens in a sequence."
      ],
      "metadata": {
        "id": "LQYK7TK1lN5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyPositionalEncoding(nn.Module):\n",
        "    \"\"\"Fixed sinusoidal positional encoding as in the paper.\"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, d_model=512, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, d_model)\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "GbCkZ-dmzHzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer"
      ],
      "metadata": {
        "id": "YgrLLcq93QWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Encoder Layer, which performs self-attention followed by a feed-forward network, using residual connections, dropout, and layer normalization to stabilize training and improve performance."
      ],
      "metadata": {
        "id": "-HjSc7BplUHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MyMultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
        "        self.ffn = MyPositionwiseFFN(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
        "        # Self-attention sublayer\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask=src_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output)) # residual + layernorm\n",
        "\n",
        "\n",
        "        # Feed-forward sublayer\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "zUdtZyhc2CON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Layer"
      ],
      "metadata": {
        "id": "rQXnLy-33giW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Decoder Layer, which performs masked self-attention, encoder-decoder (cross) attention, and a feed-forward network—each followed by residual connections, dropout, and layer normalization to maintain stable learning."
      ],
      "metadata": {
        "id": "QTieP3gVlY9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MyMultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
        "        self.cross_attn = MyMultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
        "        self.ffn = MyPositionwiseFFN(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None):\n",
        "        # Masked self-attention\n",
        "        self_attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "\n",
        "        # Cross-attention (encoder-decoder attention)\n",
        "        cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, mask=memory_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm3(x + self.dropout(ffn_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ySx_bizo3SFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (stack of N layers)"
      ],
      "metadata": {
        "id": "r0lXuZqN330w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Encoder, which stacks multiple encoder layers to process the input sequence and extract rich contextual representations."
      ],
      "metadata": {
        "id": "zHME8W5EleAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyEncoder(nn.Module):\n",
        "    def __init__(self, num_layers=6, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            MyEncoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, src, src_mask: Optional[torch.Tensor] = None):\n",
        "        x = src\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QZb0tzJN3iLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder (stack of N layers)"
      ],
      "metadata": {
        "id": "jCsxAuqX4UBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the Decoder, which stacks multiple decoder layers to generate the output sequence by attending to both previous target tokens and the encoder’s output."
      ],
      "metadata": {
        "id": "lHs5H2g0ljCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDecoder(nn.Module):\n",
        "    def __init__(self, num_layers=6, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            MyDecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, tgt, enc_output, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None):\n",
        "        x = tgt\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JadOeq6i35j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Transformer"
      ],
      "metadata": {
        "id": "bg_yJpS24rE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the full Transformer model, including token embeddings, positional encoding, stacked encoder and decoder, and a final linear layer to project decoder outputs to the target vocabulary."
      ],
      "metadata": {
        "id": "E6UKu_rbloGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size: int,\n",
        "        tgt_vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        num_heads: int = 8,\n",
        "        d_ff: int = 2048,\n",
        "        num_layers: int = 6,\n",
        "        max_seq_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_enc = MyPositionalEncoding(d_model=d_model, max_len=max_seq_len, dropout=dropout)\n",
        "\n",
        "\n",
        "        self.encoder = MyEncoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
        "        self.decoder = MyDecoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "\n",
        "        self.out_proj = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "\n",
        "        # initialize parameters as in the paper (xavier)\n",
        "        self._init_parameters()\n",
        "\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "    def forward(self, src_input, tgt_input, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None):\n",
        "        # src_input, tgt_input: (B, seq_len) -- token ids\n",
        "        src_emb = self.src_tok_emb(src_input) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.tgt_tok_emb(tgt_input) * math.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "        src_emb = self.pos_enc(src_emb)\n",
        "        tgt_emb = self.pos_enc(tgt_emb)\n",
        "\n",
        "\n",
        "        enc_output = self.encoder(src_emb, src_mask)\n",
        "        dec_output = self.decoder(tgt_emb, enc_output, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "\n",
        "\n",
        "        logits = self.out_proj(dec_output) # (B, seq_len_tgt, tgt_vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "gMhQKWdi4V5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mask helpers"
      ],
      "metadata": {
        "id": "DploIySl5PqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provides functions to create attention masks:\n",
        "\n",
        "- create_padding_mask hides padding tokens,\n",
        "\n",
        "- create_look_ahead_mask prevents attention to future tokens,\n",
        "\n",
        "- combine_masks merges padding and look-ahead masks for decoder attention."
      ],
      "metadata": {
        "id": "Ju0FZtdQlu7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq, pad_token=0):\n",
        "    \"\"\"Create a mask to hide padding tokens.\n",
        "\n",
        "\n",
        "    Returns a mask of shape (batch, 1, 1, seq_len) with 0 for real tokens and 1 for PADs.\n",
        "    But our attention expects mask with 1 for allowed? In this implementation ScaledDotProductAttention\n",
        "    masks positions where mask == 0; so we return mask where PAD positions are 0 and others are 1.\n",
        "\n",
        "\n",
        "    To be consistent, we produce a mask with 1 for allowed positions and 0 for PADs, and later in\n",
        "    ScaledDotProductAttention we invert with masked_fill(mask == 0, -inf).\n",
        "    \"\"\"\n",
        "    # seq: (B, seq_len)\n",
        "    mask = (seq != pad_token).unsqueeze(1).unsqueeze(2).to(seq.device) # (B,1,1,seq_len)\n",
        "    # For broadcasting into (B, h, seq_q, seq_k) or (B, seq_q, seq_k)\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    \"\"\"Mask out future positions. Returns (1, size, size) or (size, size) mask with 1s in allowed positions.\"\"\"\n",
        "    mask = torch.triu(torch.ones((size, size), dtype=torch.bool), diagonal=1) # upper triangular\n",
        "    # allowed positions should be True, so invert\n",
        "    return (~mask).unsqueeze(0) # (1, size, size)\n",
        "\n",
        "\n",
        "\n",
        "def combine_masks(pad_mask_src=None, pad_mask_tgt=None, look_ahead_mask=None):\n",
        "    \"\"\"Combine padding and look-ahead masks into the shape expected by attention.\n",
        "\n",
        "\n",
        "    Typical uses:\n",
        "    - src_mask for encoder self-attn: pad_mask_src -> (B,1,1,seq_len_src)\n",
        "    - tgt_mask for decoder self-attn: combined pad_mask_tgt & look_ahead -> (B,1,seq_len_tgt,seq_len_tgt)\n",
        "    - memory_mask for cross-attn: pad_mask_src broadcasted -> (B,1,1,seq_len_src) or (B,1,seq_len_tgt,seq_len_src)\n",
        "    \"\"\"\n",
        "    if pad_mask_tgt is not None and look_ahead_mask is not None:\n",
        "        # pad_mask_tgt: (B,1,1,seq_len_tgt), look_ahead_mask: (1, seq_len_tgt, seq_len_tgt)\n",
        "        # We want result shape (B, 1, seq_len_tgt, seq_len_tgt)\n",
        "        B = pad_mask_tgt.size(0)\n",
        "        seq_len = pad_mask_tgt.size(-1)\n",
        "        # squeeze pad to (B, seq_len)\n",
        "        pad = pad_mask_tgt.squeeze(1).squeeze(1).unsqueeze(2) # (B, seq_len, 1)\n",
        "        look = look_ahead_mask.to(pad_mask_tgt.device).expand(B, -1, -1) # (B, seq_len, seq_len)\n",
        "        combined = pad & look # (B, seq_len, seq_len)\n",
        "        return combined.unsqueeze(1) # (B,1,seq_len,seq_len)\n",
        "\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "3fYXkf444uiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal smoke-test / example usage"
      ],
      "metadata": {
        "id": "cPoAZFX45iNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs a test script that creates a dummy Transformer model, generates sample input sequences with padding, builds attention masks, performs a forward pass, and prints the shapes to verify the model works correctly."
      ],
      "metadata": {
        "id": "v5dN9Cm7l3FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Base model hyperparameters\n",
        "    d_model = 512\n",
        "    num_heads = 8\n",
        "    d_ff = 2048\n",
        "    num_layers = 6\n",
        "    dropout = 0.1\n",
        "\n",
        "\n",
        "    # Dummy vocabulary sizes (small for the test)\n",
        "    src_vocab_size = 1000\n",
        "    tgt_vocab_size = 1000\n",
        "\n",
        "\n",
        "    # Dummy batch and sequence lengths\n",
        "    B = 2\n",
        "    src_seq_len = 10\n",
        "    tgt_seq_len = 9\n",
        "\n",
        "\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "\n",
        "    # instantiate model\n",
        "    model = MyTransformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    num_layers=num_layers,\n",
        "    max_seq_len=512,\n",
        "    dropout=dropout,\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    # Create dummy input token IDs and masks (0 is PAD token)\n",
        "    src_input = torch.randint(1, src_vocab_size, (B, src_seq_len)).to(device)\n",
        "    tgt_input = torch.randint(1, tgt_vocab_size, (B, tgt_seq_len)).to(device)\n",
        "\n",
        "\n",
        "    # Introduce some PAD tokens for testing padding mask\n",
        "    src_input[0, -2:] = 0\n",
        "    tgt_input[1, -3:] = 0\n",
        "\n",
        "\n",
        "    src_pad_mask = create_padding_mask(src_input, pad_token=0) # (B,1,1,src_seq_len)\n",
        "    tgt_pad_mask = create_padding_mask(tgt_input, pad_token=0) # (B,1,1,tgt_seq_len)\n",
        "\n",
        "\n",
        "    look_ahead = create_look_ahead_mask(tgt_seq_len) # (1, tgt_seq_len, tgt_seq_len)\n",
        "    tgt_mask = combine_masks(pad_mask_src=None, pad_mask_tgt=tgt_pad_mask, look_ahead_mask=look_ahead) # (B,1,tgt_seq_len,tgt_seq_len)\n",
        "\n",
        "\n",
        "    # memory mask shapes need to match (B,1,1,src_seq_len) or (B,1,tgt_seq_len,src_seq_len); here we expand pad\n",
        "    memory_mask = src_pad_mask.unsqueeze(2) if src_pad_mask.dim() == 4 else src_pad_mask\n",
        "\n",
        "\n",
        "    print('src_input shape:', src_input.shape)\n",
        "    print('tgt_input shape:', tgt_input.shape)\n",
        "    print('src_pad_mask shape:', src_pad_mask.shape)\n",
        "    print('tgt_mask shape:', tgt_mask.shape)\n",
        "\n",
        "\n",
        "    logits = model(src_input, tgt_input, src_mask=src_pad_mask.squeeze(2).squeeze(2), tgt_mask=tgt_mask, memory_mask=src_pad_mask)\n",
        "    # NOTE: In our MyMultiHeadAttention we accept mask shapes that are broadcastable. For clarity we passed\n",
        "    # different shapes; the attention code will attempt to expand/broadcast.\n",
        "\n",
        "\n",
        "    print('logits shape:', logits.shape) # expected (B, tgt_seq_len, tgt_vocab_size)\n",
        "\n",
        "\n",
        "    # Quick assertions\n",
        "    assert logits.shape == (B, tgt_seq_len, tgt_vocab_size)\n",
        "    print('Successful forward pass with Base Model hyperparameters!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw9YLnZU5RSW",
        "outputId": "54b950b1-bce8-477f-dff2-31d0b979c422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_input shape: torch.Size([2, 10])\n",
            "tgt_input shape: torch.Size([2, 9])\n",
            "src_pad_mask shape: torch.Size([2, 1, 1, 10])\n",
            "tgt_mask shape: torch.Size([2, 1, 9, 9])\n",
            "logits shape: torch.Size([2, 9, 1000])\n",
            "Successful forward pass with Base Model hyperparameters!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Afbdz3mc5m41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4de7f7a"
      },
      "source": [
        "# Task\n",
        "Visualize the attention weights from the `MyMultiHeadAttention` module using `matplotlib.pyplot` and `seaborn`. This involves creating dummy input tensors for the attention mechanism, performing a forward pass to obtain the attention weights, and then generating a heatmap to display these weights for a chosen attention head and batch item, ensuring the plot has clear labels and a title."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3315ba"
      },
      "source": [
        "## Prepare for Attention Visualization\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries for plotting, such as `matplotlib.pyplot` and `seaborn`. Also, create dummy input tensors (query, key, value) and a mask to simulate the input to the `MyMultiHeadAttention` module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b58df5e"
      },
      "source": [
        "**Reasoning**:\n",
        "Importing the specified libraries for plotting and defining the necessary parameters and dummy input tensors for the `MyMultiHeadAttention` module as requested in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6d281792",
        "outputId": "11350e7c-064b-4423-fe16-6e3c81e7c2ab"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define parameters for dummy inputs\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "\n",
        "# Create dummy input tensors for MyMultiHeadAttention\n",
        "query = torch.rand(1, seq_len, d_model)\n",
        "key = torch.rand(1, seq_len, d_model)\n",
        "value = torch.rand(1, seq_len, d_model)\n",
        "\n",
        "# Create a dummy mask tensor\n",
        "mask = torch.ones(1, seq_len, seq_len)\n",
        "\n",
        "print(f\"Dummy query shape: {query.shape}\")\n",
        "print(f\"Dummy key shape: {key.shape}\")\n",
        "print(f\"Dummy value shape: {value.shape}\")\n",
        "print(f\"Dummy mask shape: {mask.shape}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-711732329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create dummy input tensors for MyMultiHeadAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ]
}